# RNN_seq2seq Repository

This directory contains example code for the the RNN / Sequence-to-sequence models. 

## Detectron2:(to be continued)
Pytorch framework implemented by the Facebook's AI Research team. It contains implementations of state-of-the-art object detection algorithms including variants of Mask R-CNN:
https://github.com/facebookresearch/detectron2

A forked copy is available here: https://github.com/ecbme6040/detectron2

### seq2seq_translation_tutorial.ipynb: 
Jupyter notebook contains the tutorial for sequence-to-sequence in Pytorch. The dataset for this tutorial could be found at <https://download.pytorch.org/tutorial/data.zip>. The tutorial used the dataset 'data/eng-fra.txt' since the English to French pairs are too big to include in the repo but feel free to try yourself for the entire dataset. 

### Additional resources:
Here's more resources for machine translation: https://github.com/ArushiSinghal/Neural-Machine-Translation-English-Hindi-for-domain-data 

## Questions?

For any questions, please feel free to contact Chelsea Cui (ac4788@columbia.edu), Tong Xue (tx2208@columbia.edu).

## References

[1] https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks

[2] https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html

[3] https://medium.com/analytics-vidhya/intuitive-understanding-of-seq2seq-model-attention-mechanism-in-deep-learning-1c1c24aace1e

[4] https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes06-NMT_seq2seq_attention.pdf

[5] [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)

[6] [Sequence to Sequence Learning with Neural Networks (original seq2seq NMT paper)](https://arxiv.org/pdf/1409.3215.pdf)

[7] [Stanford CS224N notes for Seq2Seq and Attention] (https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture08-nmt.pdf)

[8] https://medium.com/analytics-vidhya/intuitive-understanding-of-seq2seq-model-attention-mechanism-in-deep-learning-1c1c24aace1e

[9] [Effective Approaches to Attention-based Neural Machine Translation (Luong attention paper)](https://arxiv.org/abs/1508.04025)

[10] http://colah.github.io/posts/2015-08-Understanding-LSTMs/ 

[11] https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html


